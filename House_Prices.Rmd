---
title: "House Prices"
author: "Louis Dubaere"
date: "22/5/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Load the data

First, we will load the data and the different scripts that provide the necessary libraries and functions.


```{r load data and functions/libraries, message=FALSE, warning=FALSE, results='hide'}
source('load_libraries.R')
source('load_functions.R')

train <- read.csv("https://gist.githubusercontent.com/louisdubaere/946bfbcf88ec80b99bf888a56ebe708b/raw/9e3d027657f840ce793217ee8308dd71ac727bce/house_price_train.csv")
test <- read.csv("https://gist.githubusercontent.com/louisdubaere/1f36adc493ccc7018e70a54158ae5974/raw/fe88c56ef3e672e6bd4bac091b27454b9f2a1477/house_price_test.csv")

```

## Exploratory Data Analysis

We will start with the exploratory data analysis, by analyzing the type of every column in the train data set.

```{r check_types}
str(train)
head(train)
```

We can see that some columns have a doubtful type associated with it. For example 'date' is a factor and 'waterfront' is an int (while it seems a binary variable). These will later be solved.

We check if there are any missing values.

```{r check missing values}
sum(is.na(train))

```
It turns out there are no missing values in the dataset.

Now we will analyze the data visually with some graphs. We will begin with some histograms that represent the distribution of some columns, to understand the data.

```{r histograms}
hist_data <- train[, !names(train) %in% c("id", "date", "zipcode")] 

ggplot(gather(hist_data), aes(value)) + 
  geom_histogram(bins = 10) + 
  facet_wrap(~key, scales = 'free_x')
```

On the histograms, we see some columns that can be classified as 'factors'. 
These being: condition, view, waterfront. We convert these to factors

```{r factorize columns}
factors <- c("condition", "view", "waterfront")

train_factorized <- factorize_columns(train, factors)
```

Next up, we take a look at the correlation between the variables.


```{r correlation plot}
corr <- cor(train_factorized[,numerical_columns(train_factorized)])
corrplot(corr, order= 'AOE')

```

Bedrooms, sqft_living, grade, bathrooms, sqft_living15, sqft_above and sqft_basement have the highest positive correlation with the price.
We can drop some columns: zipcode, long, lat, id: don't add information about the price, they don't have a correlation with the price.

Now we will look at the boxplots for the different factors of the three factor columns (condition, view and waterfront), regarding the price.

```{r boxplots}
gathered_factors <- gather(train_factorized[,factors])
gathered_factors$price <- rep(train_factorized$price, times = 3)
ggplot(aes(y = price, x= value), data = gathered_factors) + geom_boxplot() + facet_wrap(~key, scales = 'free_x')

```

We see that, the higher the condition and the view, the higher the average price of the house. We can also see that a waterfront house has a higher average price than a non-waterfront house.

Now let's compare some variables (bathrooms, floors, grade, sqft_above, sqft_basement and sqft_living15) for different price-ranges. We create 4 equally populated price-ranges and compare these previously mentioned variables for each price range with density plots.

```{r density plots}
groups <- quant_groups(train_factorized$price, 4)
density_df <- cbind(data.frame(train_factorized), groups)

density_plot_columns <- c("bathrooms", "floors", "grade", "sqft_above", "sqft_living15", "sqft_basement")

gathered_density <- gather(density_df[,density_plot_columns])
gathered_density$group <- rep(density_df$groups, times = length(density_plot_columns))

ggplot(aes(color = group, x= value), data = gathered_density) + geom_density() + facet_wrap(~key, scales = 'free')

#ggplot(density_df, aes(x=sqft_living15, color=groups)) + geom_density()
```

We can see some interesting patterns in these density plots.

# Data Preparation

For the data preparation, we start by extracting the year, month and day from the date column. After that, we drop the zipcode, id and date column.

```{r drop_columns and add_date_info}

date_list <- strsplit(as.character(train_dropped$date), "/")
train_dropped$year <- sapply(date_list, function(x) as.character(x[3]))
train_dropped$month <- sapply(date_list, function(x) as.numeric(x[1]))
train_dropped$day <- sapply(date_list, function(x) as.numeric(x[2]))

train_dropped <- drop_columns(train_factorized, c("zipcode", "id", "date"))
```

# Baseline model

Now we will train a linear regression model on our prepared data. We split the data in 80% train and 20% test.

```{r train_test}
#splitted <- split_train_test(train_dropped)
```

We train a linear regression model to predict the price.

```{r baseline}
calculate_MAPE_lm(train_dropped)
```

The baseline MAPE score is 25.33 %.

## Feature Engineering

We will now try to do feature engineering, in order to improve the MAPE score.

First of all, we will add a column that represents the fact that a house got renovated or not. After that we add the age of the house.

```{r renovated}

train_engineered <- transform(train_dropped, renovated= ifelse(yr_renovated==0, 0, 1))
train_engineered$age <- as.numeric(train_engineered$year) - train_engineered$yr_built

```


```{r test}
x <- drop_columns(train_engineered, c("price"))
y <- train_engineered$price
lambda <- 10^seq(10, -2, length = 100)

ridge.mod <- glmnet(x, y, alpha = 0, lambda = lambda)

predict(ridge.mod, data = splitted$Train)

```

